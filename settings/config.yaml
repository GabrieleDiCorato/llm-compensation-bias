# HTTP Settings
timeout_seconds: 90

# LLM Providers Configuration
providers:
  - provider: "github"
    api_key_name: "GITHUB_TOKEN"
    url: "https://models.github.ai/inference/chat/completions"
    rate_limit_delay: 15.0     # Minimum delay in seconds between requests (throttling)
    # Generation parameters (applied to all models using this provider)
    temperature: 0.3          # Randomness (0.0-1.0): lower = more deterministic
    top_p: 0.95               # Nucleus sampling (0.0-1.0): lower = less diverse
    presence_penalty: 0.0     # Discourage new topics (-2.0 to 2.0)
    frequency_penalty: 0.2    # Discourage repetition (-2.0 to 2.0)
    # max_tokens: 4096        # Maximum tokens in response
    # stop: ["END", "STOP"]   # Stop sequences

# Available Models Configuration
models_settings:
  # OPEN AI GPT-4.1
  - model_id: "openai/gpt-4.1"
    provider: "github"
    additional_settings:
  # GPT-5-NANO
  - model_id: "openai/gpt-5-nano"
    provider: "github"
    additional_settings:
      # This model does not support custom settings
      temperature: 1.0
      top_p:
      presence_penalty:
      frequency_penalty:
      max_tokens:
  # DEEPSEEK R1-0528
  - model_id: "DeepSeek-R1-0528"
    provider: "github"
    additional_settings:
  # XAI GROK-3
  - model_id: "xai/grok-3"
    provider: "github"
    additional_settings:

# Enabled Models (subset of models_settings)
enabled_models:
  - "openai/gpt-5-nano"
  - "openai/gpt-4.1"
  #- "DeepSeek-R1-0528"
  - "xai/grok-3"

# Prompt Configuration
prompt_directory: "settings/prompts"
prompt_strategies:
  - "neutral_short"
  - "fair_short"
  - "realistic_short"
  #- "neutral"
  #- "fair"
  #- "realistic"

# Output Settings
output_dir: "src/auto_generated"